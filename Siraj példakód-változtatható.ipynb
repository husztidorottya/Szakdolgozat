{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "import helpers\n",
    "import operator\n",
    "from sklearn.model_selection import train_test_split \n",
    "from random import shuffle\n",
    "\n",
    "# GLOBAL CONTANTS\n",
    "PAD = 0\n",
    "BOS = 2\n",
    "EOS = 1\n",
    "character_changing_num = 10\n",
    "batches_in_epoch = 100\n",
    "\n",
    "# x (store encoder inputs [source morphological tags + target morphological tags + source word])\n",
    "source_data = []\n",
    "# y (store decoder expected outputs [source morphological tags + target morphological tags + target word])      \n",
    "target_data = []\n",
    "\n",
    "# stores encoded forms\n",
    "alphabet_and_morph_tags = dict()\n",
    "\n",
    "# create (source morphological tags + target morphological tags + source/target word) sequence\n",
    "def create_sequence(data_line_, word_index):\n",
    "    sequence = []\n",
    "    \n",
    "    # append beginning of the input\n",
    "    sequence.append(BOS)\n",
    "    \n",
    "    # task 2\n",
    "    if len(data_line_) == 4:\n",
    "        # source and target morphological tags are appended only to the input\n",
    "        if word_index != 3:\n",
    "            for i in data_line_[0]:\n",
    "                sequence.append(i)\n",
    "            \n",
    "            for i in data_line_[2]:\n",
    "                sequence.append(i)\n",
    "    # task 1,3\n",
    "    else:\n",
    "        if word_index != 2:\n",
    "            # source and target morphological tags are appended only to the input\n",
    "            for i in data_line_[1]:\n",
    "                sequence.append(i)\n",
    "        \n",
    "    for i in data_line_[word_index]:\n",
    "        sequence.append(i)\n",
    "        \n",
    "    # append end of the input\n",
    "    sequence.append(EOS)\n",
    "        \n",
    "    return sequence\n",
    "\n",
    "\n",
    "# encoding input data\n",
    "def encoding(data, coded_word, alphabet_and_morph_tags):\n",
    "    for character in data:\n",
    "        index = alphabet_and_morph_tags.setdefault(character, len(alphabet_and_morph_tags) + 3)\n",
    "        coded_word.append(index)\n",
    "        \n",
    "    return coded_word\n",
    "\n",
    "\n",
    "# read, split and encode input data\n",
    "with open('task2.tsv','r') as input_file:\n",
    "    idx = 0\n",
    "    # read it line-by-line\n",
    "    for line in input_file:\n",
    "        data_line_ = line.strip('\\n').split('\\t')\n",
    "        \n",
    "        # encode words into vector of ints \n",
    "        for item in range(0,len(data_line_)):         \n",
    "            # contains encoded form of word\n",
    "            coded_word = []\n",
    "            \n",
    "            # task 2\n",
    "            if len(data_line_) == 4:\n",
    "                if item == 1 or item == 3:\n",
    "                    # encode source and target word\n",
    "                    coded_word = encoding(data_line_[item], coded_word, alphabet_and_morph_tags)\n",
    "                else:\n",
    "                    # split morphological tags\n",
    "                    tags = data_line_[item].split(',')\n",
    "                \n",
    "                    coded_word = encoding(tags, coded_word, alphabet_and_morph_tags)\n",
    "            # task 1,3\n",
    "            else:\n",
    "                if item == 1:\n",
    "                    # split morphological tags\n",
    "                    tags = data_line_[item].split(',')\n",
    "                \n",
    "                    coded_word = encoding(tags, coded_word, alphabet_and_morph_tags)\n",
    "                else:\n",
    "                    # encode source and target word\n",
    "                    coded_word = encoding(data_line_[item], coded_word, alphabet_and_morph_tags)\n",
    "                        \n",
    "            # store encoded form\n",
    "            data_line_[item] = coded_word\n",
    "        \n",
    "        # defines source and target words' index\n",
    "        source_idx = len(data_line_) - 3\n",
    "        target_idx = len(data_line_) - 1 \n",
    "        \n",
    "        # store encoder input task 2:(source morphological tags + target morphological tags + source word)\n",
    "        # task 1,3: (source/target morphological tags + source word)\n",
    "        source_data.append([create_sequence(data_line_, source_idx), idx])\n",
    "        \n",
    "        # store decoder expected outputs:(target word)\n",
    "        target_data.append(create_sequence(data_line_, target_idx))\n",
    "        \n",
    "        # stores line number (needed for shuffle) - reference for the target_data\n",
    "        idx += 1\n",
    "        \n",
    "# split data into train and test sets\n",
    "source_data_train, source_data_test, target_data_train, target_data_test = train_test_split(source_data, target_data, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A programhoz felhasznált forrásfájl a következőképpen néz ki:\n",
    "    \n",
    "    task1 esetén:  (forrás szóalak + cél morfológiai tagek + cél szóalak)\n",
    "    agyondicsér\tpos=V,mood=IND,def=DEF,tense=PST,per=1,num=PL\tagyondicsértük\n",
    "    ürít\tpos=V,mood=POT\türíthet\n",
    "    vitat\tpos=V,polite=INFM,per=2,num=SG,finite=NFIN\tvitatnod\n",
    "     \n",
    "    task 2 esetén: (forrás morfológiai tagek + forrás szóalak + cél morfológiai tagek + cél szóalak)\n",
    "    pos=V,mood=IND,def=INDF,tense=PRS,per=1,num=PL\tagyondicsérünk\tpos=V,mood=IND,def=DEF,tense=PST,per=1,num=PL\tagyondicsértük\n",
    "    pos=V,tense=PRS\türítő\tpos=V,mood=POT\türíthet\n",
    "    pos=V,finite=NFIN\tvitatni\tpos=V,polite=INFM,per=2,num=SG,finite=NFIN\tvitatnod\n",
    "    \n",
    "    task 3 esetén:  (forrás szóalak + forrás morfológiai tagek + cél szóalak)\n",
    "    agyondicsérünk\tpos=V,mood=IND,def=DEF,tense=PST,per=1,num=PL\tagyondicsértük\n",
    "    ürítő\tpos=V,mood=POT\türíthet\n",
    "    vitatni\tpos=V,polite=INFM,per=2,num=SG,finite=NFIN\tvitatnod\n",
    " \n",
    "Minden sora egy-egy bemeneti adatot reprezentál.\n",
    "\n",
    "Beolvassuk a fájlból soronként, elvégezzük a szükséges feldarabolási lépéseket, majd kódoljuk mind a morfológiai tageket, mind a szóalakokat is számok formájában. Ehhez a már megszokott +1-gyel növelt kódolást használja. A morfológiai tageket páronként kódolja szintén mindig +1-gyel növelt értéktől kezdve: (FONTOS! mivel EOS=1 és PAD=0 és BOS=2 ezért a kódolást a 3-as értéktől kezdi)\n",
    "    \n",
    "    pl.: POS=V -> 3-as érték\n",
    "         MOOD=IND -> 4-es érték\n",
    "         \n",
    "A morfológiai tagek kódolt formájából szekvenciát épít és mind a forrás mind a cél tagek szekvenciáját a forrás szóalak szekvenciája elé fűzi (ha van), így állítja elő az source_data változóba az encoder bemenetét. \n",
    "             (BOS + forrás morfológiai tagek szekvenciája + cél morfológiai tagek szekvenciája + forrás szóalak szekvenciája + EOS)\n",
    "             \n",
    "Az előbbihez hasonlóan készíti el a decoder elvárt kimenetét is, amit a target_data változóban tárol:\n",
    "             (BOS + cél szóalak szekvenciája + EOS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Clears the default graph stack and resets the global default graph.\n",
    "tf.reset_default_graph() \n",
    "# initializes a tensorflow session\n",
    "sess = tf.InteractiveSession() \n",
    "\n",
    "# get max value of encoded forms\n",
    "max_alphabet_and_morph_tags = alphabet_and_morph_tags[max(alphabet_and_morph_tags.items(), key=operator.itemgetter(1))[0]]\n",
    "\n",
    "# calculate vocab_size\n",
    "vocab_size = max_alphabet_and_morph_tags + 1\n",
    "#character length\n",
    "input_embedding_size = 300 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A vocab_size-ot manuálisan kell kiszámolni, hogy pontosan megállapíthassuk, hogy hány különböző kódolt karakterünk van. (Ennek pontos értékére az embedding miatt van szükség)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# num neurons\n",
    "encoder_hidden_units = 100 \n",
    "# in original paper, they used same number of neurons for both encoder\n",
    "# and decoder, but we use twice as many so decoded output is different, the target value is the original input \n",
    "#in this example\n",
    "decoder_hidden_units = encoder_hidden_units * 2\n",
    "\n",
    "# input placehodlers\n",
    "encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "# contains the lengths for each of the sequence in the batch, we will pad so all the same\n",
    "# if you don't want to pad, check out dynamic memory networks to input variable length sequences\n",
    "encoder_inputs_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='encoder_inputs_length')\n",
    "decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')\n",
    "\n",
    "# randomly initialized embedding matrrix that can fit input sequence\n",
    "# used to convert sequences to vectors (embeddings) for both encoder and decoder of the right size\n",
    "# reshaping is a thing, in TF you gotta make sure you tensors are the right shape (num dimensions)\n",
    "embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)\n",
    "\n",
    "# this thing could get huge in a real world application\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)\n",
    "\n",
    "# define encoder\n",
    "encoder_cell = tf.contrib.rnn.GRUCell(encoder_hidden_units)\n",
    "\n",
    "# define bidirectionel function of encoder (backpropagation)\n",
    "((encoder_fw_outputs,\n",
    "  encoder_bw_outputs),\n",
    " (encoder_fw_final_state,\n",
    "  encoder_bw_final_state)) = (\n",
    "    tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,\n",
    "                                    cell_bw=encoder_cell,\n",
    "                                    inputs=encoder_inputs_embedded,\n",
    "                                    sequence_length=encoder_inputs_length,\n",
    "                                    dtype=tf.float32, time_major=True)\n",
    "    )\n",
    "\n",
    "#Concatenates tensors along one dimension.\n",
    "encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2)\n",
    "\n",
    "\n",
    "#letters h and c are commonly used to denote \"output value\" and \"cell state\". \n",
    "#http://colah.github.io/posts/2015-08-Understanding-LSTMs/ \n",
    "#Those tensors represent combined internal state of the cell, and should be passed together. \n",
    "\n",
    "# because by GRUCells the state is a Tensor, not a Tuple like by LSTMCells\n",
    "encoder_final_state = tf.concat(\n",
    "    (encoder_fw_final_state, encoder_bw_final_state), 1)\n",
    "\n",
    "'''\n",
    "encoder_final_state_h = tf.concat(\n",
    "    (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "\n",
    "#TF Tuple used by LSTM Cells for state_size, zero_state, and output state.\n",
    "encoder_final_state = tf.contrib.rnn.GRUStateTuple(\n",
    "    c=encoder_final_state_c,\n",
    "    h=encoder_final_state_h\n",
    ")\n",
    "'''\n",
    "\n",
    "decoder_cell = tf.contrib.rnn.GRUCell(decoder_hidden_units)\n",
    "\n",
    "#we could print this, won't need\n",
    "encoder_max_time, batch_size = tf.unstack(tf.shape(encoder_inputs))\n",
    "\n",
    "decoder_lengths = encoder_inputs_length + character_changing_num\n",
    "# +(character_changing_num-1) additional steps, +1 leading <EOS> token for decoder inputs\n",
    "\n",
    "#manually specifying since we are going to implement attention details for the decoder in a sec\n",
    "#weights\n",
    "W = tf.Variable(tf.random_uniform([decoder_hidden_units, vocab_size], -1, 1), dtype=tf.float32)\n",
    "#bias\n",
    "b = tf.Variable(tf.zeros([vocab_size]), dtype=tf.float32)\n",
    "\n",
    "#create padded inputs for the decoder from the word embeddings\n",
    "#were telling the program to test a condition, and trigger an error if the condition is false.\n",
    "assert EOS == 1 and PAD == 0 and BOS == 2\n",
    "\n",
    "bos_time_slice = tf.ones([batch_size], dtype=tf.int32, name='BOS')\n",
    "eos_time_slice = tf.ones([batch_size], dtype=tf.int32, name='EOS')\n",
    "pad_time_slice = tf.zeros([batch_size], dtype=tf.int32, name='PAD')\n",
    "\n",
    "#retrieves rows of the params tensor. The behavior is similar to using indexing with arrays in numpy\n",
    "bos_step_embedded = tf.nn.embedding_lookup(embeddings, bos_time_slice)\n",
    "eos_step_embedded = tf.nn.embedding_lookup(embeddings, eos_time_slice)\n",
    "pad_step_embedded = tf.nn.embedding_lookup(embeddings, pad_time_slice)\n",
    "\n",
    "#manually specifying loop function through time - to get initial cell state and input to RNN\n",
    "#normally we'd just use dynamic_rnn, but lets get detailed here with raw_rnn\n",
    "\n",
    "#we define and return these values, no operations occur here\n",
    "def loop_fn_initial():\n",
    "    initial_elements_finished = (0 >= decoder_lengths)  # all False at the initial step\n",
    "    #end of sentence\n",
    "    initial_input = eos_step_embedded\n",
    "    #last time steps cell state\n",
    "    initial_cell_state = encoder_final_state\n",
    "    #none\n",
    "    initial_cell_output = None\n",
    "    #none\n",
    "    initial_loop_state = None  # we don't need to pass any additional information\n",
    "    return (initial_elements_finished,\n",
    "            initial_input,\n",
    "            initial_cell_state,\n",
    "            initial_cell_output,\n",
    "            initial_loop_state)\n",
    "\n",
    "\n",
    "#attention mechanism --choose which previously generated token to pass as input in the next timestep\n",
    "def loop_fn_transition(time, previous_output, previous_state, previous_loop_state):\n",
    "\n",
    "    def get_next_input():\n",
    "        #dot product between previous ouput and weights, then + biases\n",
    "        output_logits = tf.add(tf.matmul(previous_output, W), b)\n",
    "        #Logits simply means that the function operates on the unscaled output of \n",
    "        #earlier layers and that the relative scale to understand the units is linear. \n",
    "        #It means, in particular, the sum of the inputs may not equal 1, that the values are not probabilities \n",
    "        #(you might have an input of 5).\n",
    "        #prediction value at current time step\n",
    "        \n",
    "        #Returns the index with the largest value across axes of a tensor.\n",
    "        prediction = tf.argmax(output_logits, axis=1)\n",
    "        #embed prediction for the next input\n",
    "        next_input = tf.nn.embedding_lookup(embeddings, prediction)\n",
    "        return next_input\n",
    "    \n",
    "    \n",
    "    elements_finished = (time >= decoder_lengths) # this operation produces boolean tensor of [batch_size]\n",
    "                                                  # defining if corresponding sequence has ended\n",
    "\n",
    "    \n",
    "    #Computes the \"logical and\" of elements across dimensions of a tensor.\n",
    "    finished = tf.reduce_all(elements_finished) # -> boolean scalar\n",
    "    #Return either fn1() or fn2() based on the boolean predicate pred.\n",
    "    input = tf.cond(finished, lambda: pad_step_embedded, get_next_input)\n",
    "    \n",
    "    #set previous to current\n",
    "    state = previous_state\n",
    "    output = previous_output\n",
    "    loop_state = None\n",
    "\n",
    "    return (elements_finished, \n",
    "            input,\n",
    "            state,\n",
    "            output,\n",
    "            loop_state)\n",
    "\n",
    "def loop_fn(time, previous_output, previous_state, previous_loop_state):\n",
    "    if previous_state is None:    # time == 0\n",
    "        assert previous_output is None and previous_state is None\n",
    "        return loop_fn_initial()\n",
    "    else:\n",
    "        return loop_fn_transition(time, previous_output, previous_state, previous_loop_state)\n",
    "\n",
    "#Creates an RNN specified by RNNCell cell and loop function loop_fn.\n",
    "#This function is a more primitive version of dynamic_rnn that provides more direct access to the \n",
    "#inputs each iteration. It also provides more control over when to start and finish reading the sequence, \n",
    "#and what to emit for the output.\n",
    "#ta = tensor array\n",
    "decoder_outputs_ta, decoder_final_state, _ = tf.nn.raw_rnn(decoder_cell, loop_fn)\n",
    "# emiatt nem lehet lefelezni decoder hidden unit számot DE MIÉRT??? (talán azért, mert bidirekciónál a fw és bw is encoder_hidden_units ezért lesz kétszeres)\n",
    "\n",
    "decoder_outputs = decoder_outputs_ta.stack()\n",
    "\n",
    "decoder_outputs\n",
    "\n",
    "#to convert output to human readable prediction\n",
    "#we will reshape output tensor\n",
    "\n",
    "#Unpacks the given dimension of a rank-R tensor into rank-(R-1) tensors.\n",
    "#reduces dimensionality\n",
    "decoder_max_steps, decoder_batch_size, decoder_dim = tf.unstack(tf.shape(decoder_outputs))\n",
    "#flettened output tensor\n",
    "decoder_outputs_flat = tf.reshape(decoder_outputs, (-1, decoder_dim))\n",
    "#pass flattened tensor through decoder\n",
    "decoder_logits_flat = tf.add(tf.matmul(decoder_outputs_flat, W), b)\n",
    "#prediction vals\n",
    "decoder_logits = tf.reshape(decoder_logits_flat, (decoder_max_steps, decoder_batch_size, vocab_size))\n",
    "\n",
    "#final prediction\n",
    "decoder_prediction = tf.argmax(decoder_logits, 2)\n",
    "\n",
    "#cross entropy loss\n",
    "#one hot encode the target values so we don't rank just differentiate\n",
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32),\n",
    "    logits=decoder_logits,\n",
    ")\n",
    "\n",
    "#loss function\n",
    "loss = tf.reduce_mean(stepwise_cross_entropy)\n",
    "#train it \n",
    "#train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "train_op = tf.train.GradientDescentOptimizer(0.1).minimize(loss) # set learning_rate = 0.1\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# send 20 sequences into encoder at one time\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create batches with size of batch_size\n",
    "def create_batches(source_data, target_data,batch_size):\n",
    "    # stores batches\n",
    "    source_batches = []\n",
    "    target_batches = []\n",
    "    # stores last batch ending index\n",
    "    prev_batch_end = 0\n",
    "    \n",
    "    for j in range(0, len(source_data)):\n",
    "        if j % batch_size == 0 and j != 0:\n",
    "            # stores a batch\n",
    "            sbatch = []\n",
    "            tbatch = []\n",
    "            for k in range(prev_batch_end+1,j):\n",
    "                # store sequence\n",
    "                sbatch.append(source_data[k][0])\n",
    "                # store expected target_data (know from source_data index)\n",
    "                tbatch.append(target_data[source_data[k][1]])\n",
    "            # add created batch\n",
    "            source_batches.append(sbatch)\n",
    "            target_batches.append(tbatch)\n",
    "            prev_batch_end = j\n",
    "            \n",
    "    # put the rest of it in another batch\n",
    "    if prev_batch_end != j:\n",
    "        sbatch = []\n",
    "        tbatch = []\n",
    "        for k in range(prev_batch_end+1,j):\n",
    "            sbatch.append(source_data[k][0])\n",
    "            tbatch.append(target_data[source_data[k][1]])\n",
    "        source_batches.append(sbatch)\n",
    "        target_batches.append(tbatch)\n",
    "        \n",
    "    return source_batches, target_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A bemeneti adatokból és az elvárt kimenetekből legyártja a batch_size-nak megfelelő méretű batcheket. \n",
    "\n",
    "Maga a batch szekvenciák kötegét jelenti, hogy egyszerre hány input sort adunk be a rendszerünknek. Ezért fontos, hogy mind az encoder bemenetén, mind a decoder kimenetén azonos méretű batchek legyenek. Emiatt hívjuk meg a source_data és target_data-ra is egyaránt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def next_feed(batch_num, source_batches, target_batches):\n",
    "    # get transpose of source_batches[batch_num]\n",
    "    encoder_inputs_, encoder_input_lengths_ = helpers.batch(source_batches[batch_num])\n",
    "    \n",
    "    # get max input sequence length\n",
    "    max_input_length = max(encoder_input_lengths_)\n",
    "    \n",
    "    # target word is max character_changing_num character longer than source word \n",
    "    # get transpose of target_batches[i] and put an EOF and PAD at the end\n",
    "    decoder_targets_, _ = helpers.batch(\n",
    "            [(sequence) + [PAD] * ((max_input_length + character_changing_num) - len(sequence))  for sequence in target_batches[batch_num]]\n",
    "    )\n",
    "   \n",
    "    return {\n",
    "        encoder_inputs: encoder_inputs_,\n",
    "        encoder_inputs_length: encoder_input_lengths_,\n",
    "        decoder_targets: decoder_targets_,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Mivel a forrás szóalak hossza nem feltétlenül egyezik meg a cél szóalak hosszával, ezért fontos hogy lehetővé tegyük a rendszer számára, hogy további karaktereket fűzhessen az eredetihez. Azt, hogy hány karakterrel lehet hosszabb a képzett szó (cél szó) az character_changing_num változó definiálja.\n",
    "\n",
    "    Ha a character_changing_num = 10 ez azt jelenti, hogy 10 karakterben térhet el az eredeti szóalaktól.\n",
    "    \n",
    "Az ehhez szükséges padding karakterek számának kiszámolásához megkeressük a legnagyobb bemeneti szekvencia hosszát, amit a max_input_length változóban tárolunk el. Ezután a legnagyobb bemeneti szekvencia hosszához hozzáadjuk a (character_changing_num) értéket és kivonjuk belőle az aktuális szekvencia hosszát. Ezzel az ettől való eltéréseket 0-val töltjük fel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "  minibatch loss: 3.285431146621704\n",
      "  sample 1:\n",
      "    input     > [ 2  3  4 22 32 23 33  8  3  4  5 23  7  8 29 37 28 15 24 24 18 24 29 21  1\n",
      "  0  0  0  0  0  0]\n",
      "    predicted > [ 3  0  0  0  0  0  0  0  0 40  0  0  0  0  0  0  0 40  0  0  0  0  0  0  0\n",
      " 40  0  0  0  0  0  0  0 40  0  0  0  0  0  0  0]\n",
      "  sample 2:\n",
      "    input     > [ 2  3  4  5  6  7  8  3  4  5 32  6 33  8 31 29 17 40 24 29 10 29 24 20 13\n",
      " 21  1  0  0  0  0]\n",
      "    predicted > [ 3  0  0  0  0  0  0  0  0 40  0  0  0  0  0  0  0 40  0  0  0  0  0  0  0\n",
      " 40  0  0  0  0  0  0  0 40  0  0  0  0  0  0  0]\n",
      "  sample 3:\n",
      "    input     > [ 2  3 23  3 35  5 32  6 33 34 24  9 21  9 19 46 40 12 24 24  1  0  0  0  0\n",
      "  0  0  0  0  0  0]\n",
      "    predicted > [ 3  0  0  0  0  0  0  0  0 40  0  0  0  0  0  0  0 40  0  0  0  0  0  0  0\n",
      " 40  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "indices[19,5] = 76 is not in [0, 76)\n\t [[Node: embedding_lookup = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@Variable\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable/read, _recv_encoder_inputs_0)]]\n\nCaused by op 'embedding_lookup', defined at:\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-297-8a12ed11dfe1>\", line 22, in <module>\n    encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/embedding_ops.py\", line 111, in embedding_lookup\n    validate_indices=validate_indices)\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1359, in gather\n    validate_indices=validate_indices, name=name)\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): indices[19,5] = 76 is not in [0, 76)\n\t [[Node: embedding_lookup = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@Variable\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable/read, _recv_encoder_inputs_0)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    468\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    470\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[19,5] = 76 is not in [0, 76)\n\t [[Node: embedding_lookup = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@Variable\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable/read, _recv_encoder_inputs_0)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-300-bd7f35c72e8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_feed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mloss_track\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[19,5] = 76 is not in [0, 76)\n\t [[Node: embedding_lookup = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@Variable\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable/read, _recv_encoder_inputs_0)]]\n\nCaused by op 'embedding_lookup', defined at:\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-297-8a12ed11dfe1>\", line 22, in <module>\n    encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/embedding_ops.py\", line 111, in embedding_lookup\n    validate_indices=validate_indices)\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1359, in gather\n    validate_indices=validate_indices, name=name)\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/dorottyahuszti/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): indices[19,5] = 76 is not in [0, 76)\n\t [[Node: embedding_lookup = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@Variable\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable/read, _recv_encoder_inputs_0)]]\n"
     ]
    }
   ],
   "source": [
    "loss_track = []\n",
    "\n",
    "try:\n",
    "    # iteration number during training\n",
    "    epoch = 100\n",
    "    for j in range(0,epoch):\n",
    "        # shuffle it in every epoch for creating random batches\n",
    "        source_data_train = random.sample(source_data_train, len(source_data_train))\n",
    "        \n",
    "        # encoder inputs and decoder outputs devided into batches\n",
    "        source_batches, target_batches = create_batches(source_data_train, target_data, batch_size)\n",
    "\n",
    "        # get every batches and train the model on it\n",
    "        for batch_num in range(0, len(source_batches)):\n",
    "            fd = next_feed(batch_num, source_batches, target_batches)\n",
    "   \n",
    "            _, l = sess.run([train_op, loss], fd)\n",
    "            loss_track.append(l)\n",
    "        \n",
    "            if batch_num == 0 or batch_num % batches_in_epoch == 0:\n",
    "                print('batch {}'.format(batch_num))\n",
    "                print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "                predict_ = sess.run(decoder_prediction, fd)\n",
    "                for i, (inp, pred) in enumerate(zip(fd[encoder_inputs].T, predict_.T)):\n",
    "                    print('  sample {}:'.format(i + 1))\n",
    "                    print('    input     > {}'.format(inp))\n",
    "                    print('    predicted > {}'.format(pred))\n",
    "                    if i >= 2:\n",
    "                        break\n",
    "                print()\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
