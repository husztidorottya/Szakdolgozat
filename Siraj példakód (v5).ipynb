{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "import helpers\n",
    "import operator\n",
    "\n",
    "# GLOBAL CONTANTS\n",
    "PAD = 0\n",
    "EOS = 1\n",
    "character_changing_num = 10\n",
    "max_batches = 3001\n",
    "batches_in_epoch = 100\n",
    "# send 10 sequences into encoder at one time\n",
    "batch_size = 10\n",
    "\n",
    "# x (store encoder inputs [source morphological tags + target morphological tags + source word])\n",
    "source_data = []\n",
    "# y (store decoder expected outputs [source morphological tags + target morphological tags + target word])      \n",
    "target_data = []\n",
    "\n",
    "# character encodings\n",
    "alphabet = dict()\n",
    "\n",
    "# source and target morphological tag encodings\n",
    "morphological_tags = dict()\n",
    "\n",
    "# create (source morphological tags + target morphological tags + source/target word) sequence\n",
    "def create_sequence(data_line_, word_index):\n",
    "    sequence = []\n",
    "    \n",
    "    for i in data_line_[0]:\n",
    "        sequence.append(i)\n",
    "            \n",
    "    for i in data_line_[2]:\n",
    "        sequence.append(i)\n",
    "        \n",
    "    for i in data_line_[word_index]:\n",
    "        sequence.append(i)\n",
    "        \n",
    "    return sequence\n",
    "    \n",
    "\n",
    "# read, split and encode input data\n",
    "with open('data2.tsv','r') as input_file:\n",
    "    # read it line-by-line\n",
    "    for line in input_file:\n",
    "        data_line_ = line.strip('\\n').split('\\t')\n",
    "        \n",
    "        # encode words into vector of ints \n",
    "        for item in range(0,4):         \n",
    "            # contains encoded form of word\n",
    "            coded_word = []\n",
    "            \n",
    "            if item == 1 or item == 3:\n",
    "                # encode source and target word\n",
    "                for character in data_line_[item]:\n",
    "                    index = alphabet.setdefault(character, len(alphabet) + 2)\n",
    "                    coded_word.append(index)\n",
    "            else:\n",
    "                # split morphological tags\n",
    "                tags = data_line_[item].split(',')\n",
    "                \n",
    "                # encode morphological tags\n",
    "                for tag in tags:\n",
    "                    index = morphological_tags.setdefault(tag, len(morphological_tags) + 2)\n",
    "                    coded_word.append(index)\n",
    "            \n",
    "            # store encoded form\n",
    "            data_line_[item] = coded_word\n",
    "        \n",
    "        # store encoder input (source morphological tags + target morphological tags + source word)\n",
    "        source_data.append(create_sequence(data_line_, 1))\n",
    "        \n",
    "        # store decoder expected outputs (source morphological tags + target morphological tags + target word)\n",
    "        target_data.append(create_sequence(data_line_, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A programhoz felhasznált forrásfájl a következőképpen néz ki:\n",
    "\n",
    "    pos=V,mood=IND,def=INDF,tense=PRS,per=1,num=PL\tagyondicsérünk\t\n",
    "    pos=V,mood=IND,def=DEF,tense=PST,per=1,num=PL\tagyondicsértük\n",
    "    pos=V,tense=PRS\türítő\tpos=V,mood=POT\türíthet\n",
    "    pos=V,finite=NFIN\tvitatni\tpos=V,polite=INFM,per=2,num=SG,finite=NFIN\tvitatnod\n",
    "\n",
    "Minden sora egy-egy bemeneti adatot reprezentál (forrás morfológiai tagek + forrás szóalak + cél morfológiai tagek + cél szóalak) formában.\n",
    "\n",
    "Beolvassuk a fájlból soronként, elvégezzük a szükséges feldarabolási lépéseket, majd kódoljuk mind a morfológiai tageket, mind a szóalakokat is számok formájában. Ehhez a már megszokott +1-gyel növelt ABC kódolást használja. A morfológiai tageket páronként kódolja szintén mindig +1-gyel növelt értéktől kezdve: (FONTOS! mivel EOS=1 és PAD=0 ezért a kódolást a 2-es értéktől kezdi)\n",
    "    \n",
    "    pl.: POS=V -> 2-es érték\n",
    "         MOOD=IND -> 3-es érték\n",
    "         \n",
    "A morfológiai tagek kódolt formájából szekvenciát épít és mind a forrás mind a cél tagek szekvenciáját a forrás szóalak szekvenciája elé fűzi, így állítja elő az source_data változóba az encoder bemenetét. \n",
    "             (forrás morfológiai tagek szekvenciája + cél morfológiai tagek szekvenciája + forrás szóalak szekvenciája)\n",
    "             \n",
    "Az előbbihez hasonlóan készíti el a decoder elvárt kimenetét is, amit a target_data változóban tárol:\n",
    "             (forrás morfológiai tagek szekvenciája + cél morfológiai tagek szekvenciája + cél szóalak szekvenciája)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Clears the default graph stack and resets the global default graph.\n",
    "tf.reset_default_graph() \n",
    "# initializes a tensorflow session\n",
    "sess = tf.InteractiveSession() \n",
    "\n",
    "max_alphabet = alphabet[max(alphabet.items(), key=operator.itemgetter(1))[0]]\n",
    "max_morphological_tags = morphological_tags[max(morphological_tags.items(), key=operator.itemgetter(1))[0]]\n",
    "\n",
    "# calculate vocab_size (max(alphabet,morphological_tags))\n",
    "vocab_size = max([max_alphabet, max_morphological_tags]) + 1\n",
    "#character length\n",
    "input_embedding_size = 30 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A vocab_size-ot manuálisan kell kiszámolni, hogy pontosan megállapíthassuk, hogy hány különböző kódolt karakterünk van. (Ennek pontos értékére az embedding miatt van szükség)\n",
    "\n",
    "Mivel az ABC betűit és a morfológiai tag párok kódolt alakját is külön-külön tároltam, ezért meg kell vizsgálnom az ABC betűinél melyik a legnagyobb kódolt érték és melyik a legnagyobb kódolt érték a morfológiai tag-ek kódolt alakjánál. Ezután a két maximum érték közül a nagyobbat kell vennem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# num neurons\n",
    "encoder_hidden_units = 20 \n",
    "# in original paper, they used same number of neurons for both encoder\n",
    "# and decoder, but we use twice as many so decoded output is different, the target value is the original input \n",
    "#in this example\n",
    "decoder_hidden_units = encoder_hidden_units * 2 \n",
    "\n",
    "# input placehodlers\n",
    "encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "# contains the lengths for each of the sequence in the batch, we will pad so all the same\n",
    "# if you don't want to pad, check out dynamic memory networks to input variable length sequences\n",
    "encoder_inputs_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='encoder_inputs_length')\n",
    "decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')\n",
    "\n",
    "# randomly initialized embedding matrrix that can fit input sequence\n",
    "# used to convert sequences to vectors (embeddings) for both encoder and decoder of the right size\n",
    "# reshaping is a thing, in TF you gotta make sure you tensors are the right shape (num dimensions)\n",
    "embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)\n",
    "\n",
    "# this thing could get huge in a real world application\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)\n",
    "\n",
    "# define encoder\n",
    "encoder_cell = tf.contrib.rnn.LSTMCell(encoder_hidden_units)\n",
    "\n",
    "# define bidirectionel function of encoder (backpropagation)\n",
    "((encoder_fw_outputs,\n",
    "  encoder_bw_outputs),\n",
    " (encoder_fw_final_state,\n",
    "  encoder_bw_final_state)) = (\n",
    "    tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,\n",
    "                                    cell_bw=encoder_cell,\n",
    "                                    inputs=encoder_inputs_embedded,\n",
    "                                    sequence_length=encoder_inputs_length,\n",
    "                                    dtype=tf.float32, time_major=True)\n",
    "    )\n",
    "\n",
    "#Concatenates tensors along one dimension.\n",
    "encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2)\n",
    "\n",
    "#letters h and c are commonly used to denote \"output value\" and \"cell state\". \n",
    "#http://colah.github.io/posts/2015-08-Understanding-LSTMs/ \n",
    "#Those tensors represent combined internal state of the cell, and should be passed together. \n",
    "\n",
    "encoder_final_state_c = tf.concat(\n",
    "    (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n",
    "\n",
    "encoder_final_state_h = tf.concat(\n",
    "    (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "\n",
    "#TF Tuple used by LSTM Cells for state_size, zero_state, and output state.\n",
    "encoder_final_state = tf.contrib.rnn.LSTMStateTuple(\n",
    "    c=encoder_final_state_c,\n",
    "    h=encoder_final_state_h\n",
    ")\n",
    "\n",
    "decoder_cell = tf.contrib.rnn.LSTMCell(decoder_hidden_units)\n",
    "\n",
    "#we could print this, won't need\n",
    "encoder_max_time, batch_size = tf.unstack(tf.shape(encoder_inputs))\n",
    "\n",
    "decoder_lengths = encoder_inputs_length + character_changing_num\n",
    "# +(character_changing_num-1) additional steps, +1 leading <EOS> token for decoder inputs\n",
    "\n",
    "#manually specifying since we are going to implement attention details for the decoder in a sec\n",
    "#weights\n",
    "W = tf.Variable(tf.random_uniform([decoder_hidden_units, vocab_size], -1, 1), dtype=tf.float32)\n",
    "#bias\n",
    "b = tf.Variable(tf.zeros([vocab_size]), dtype=tf.float32)\n",
    "\n",
    "#create padded inputs for the decoder from the word embeddings\n",
    "#were telling the program to test a condition, and trigger an error if the condition is false.\n",
    "assert EOS == 1 and PAD == 0\n",
    "\n",
    "eos_time_slice = tf.ones([batch_size], dtype=tf.int32, name='EOS')\n",
    "pad_time_slice = tf.zeros([batch_size], dtype=tf.int32, name='PAD')\n",
    "\n",
    "#retrieves rows of the params tensor. The behavior is similar to using indexing with arrays in numpy\n",
    "eos_step_embedded = tf.nn.embedding_lookup(embeddings, eos_time_slice)\n",
    "pad_step_embedded = tf.nn.embedding_lookup(embeddings, pad_time_slice)\n",
    "\n",
    "#manually specifying loop function through time - to get initial cell state and input to RNN\n",
    "#normally we'd just use dynamic_rnn, but lets get detailed here with raw_rnn\n",
    "\n",
    "#we define and return these values, no operations occur here\n",
    "def loop_fn_initial():\n",
    "    initial_elements_finished = (0 >= decoder_lengths)  # all False at the initial step\n",
    "    #end of sentence\n",
    "    initial_input = eos_step_embedded\n",
    "    #last time steps cell state\n",
    "    initial_cell_state = encoder_final_state\n",
    "    #none\n",
    "    initial_cell_output = None\n",
    "    #none\n",
    "    initial_loop_state = None  # we don't need to pass any additional information\n",
    "    return (initial_elements_finished,\n",
    "            initial_input,\n",
    "            initial_cell_state,\n",
    "            initial_cell_output,\n",
    "            initial_loop_state)\n",
    "\n",
    "\n",
    "#attention mechanism --choose which previously generated token to pass as input in the next timestep\n",
    "def loop_fn_transition(time, previous_output, previous_state, previous_loop_state):\n",
    "\n",
    "    def get_next_input():\n",
    "        #dot product between previous ouput and weights, then + biases\n",
    "        output_logits = tf.add(tf.matmul(previous_output, W), b)\n",
    "        #Logits simply means that the function operates on the unscaled output of \n",
    "        #earlier layers and that the relative scale to understand the units is linear. \n",
    "        #It means, in particular, the sum of the inputs may not equal 1, that the values are not probabilities \n",
    "        #(you might have an input of 5).\n",
    "        #prediction value at current time step\n",
    "        \n",
    "        #Returns the index with the largest value across axes of a tensor.\n",
    "        prediction = tf.argmax(output_logits, axis=1)\n",
    "        #embed prediction for the next input\n",
    "        next_input = tf.nn.embedding_lookup(embeddings, prediction)\n",
    "        return next_input\n",
    "    \n",
    "    \n",
    "    elements_finished = (time >= decoder_lengths) # this operation produces boolean tensor of [batch_size]\n",
    "                                                  # defining if corresponding sequence has ended\n",
    "\n",
    "    \n",
    "    #Computes the \"logical and\" of elements across dimensions of a tensor.\n",
    "    finished = tf.reduce_all(elements_finished) # -> boolean scalar\n",
    "    #Return either fn1() or fn2() based on the boolean predicate pred.\n",
    "    input = tf.cond(finished, lambda: pad_step_embedded, get_next_input)\n",
    "    \n",
    "    #set previous to current\n",
    "    state = previous_state\n",
    "    output = previous_output\n",
    "    loop_state = None\n",
    "\n",
    "    return (elements_finished, \n",
    "            input,\n",
    "            state,\n",
    "            output,\n",
    "            loop_state)\n",
    "\n",
    "def loop_fn(time, previous_output, previous_state, previous_loop_state):\n",
    "    if previous_state is None:    # time == 0\n",
    "        assert previous_output is None and previous_state is None\n",
    "        return loop_fn_initial()\n",
    "    else:\n",
    "        return loop_fn_transition(time, previous_output, previous_state, previous_loop_state)\n",
    "\n",
    "#Creates an RNN specified by RNNCell cell and loop function loop_fn.\n",
    "#This function is a more primitive version of dynamic_rnn that provides more direct access to the \n",
    "#inputs each iteration. It also provides more control over when to start and finish reading the sequence, \n",
    "#and what to emit for the output.\n",
    "#ta = tensor array\n",
    "decoder_outputs_ta, decoder_final_state, _ = tf.nn.raw_rnn(decoder_cell, loop_fn)\n",
    "decoder_outputs = decoder_outputs_ta.stack()\n",
    "\n",
    "decoder_outputs\n",
    "\n",
    "#to convert output to human readable prediction\n",
    "#we will reshape output tensor\n",
    "\n",
    "#Unpacks the given dimension of a rank-R tensor into rank-(R-1) tensors.\n",
    "#reduces dimensionality\n",
    "decoder_max_steps, decoder_batch_size, decoder_dim = tf.unstack(tf.shape(decoder_outputs))\n",
    "#flettened output tensor\n",
    "decoder_outputs_flat = tf.reshape(decoder_outputs, (-1, decoder_dim))\n",
    "#pass flattened tensor through decoder\n",
    "decoder_logits_flat = tf.add(tf.matmul(decoder_outputs_flat, W), b)\n",
    "#prediction vals\n",
    "decoder_logits = tf.reshape(decoder_logits_flat, (decoder_max_steps, decoder_batch_size, vocab_size))\n",
    "\n",
    "#final prediction\n",
    "decoder_prediction = tf.argmax(decoder_logits, 2)\n",
    "\n",
    "#cross entropy loss\n",
    "#one hot encode the target values so we don't rank just differentiate\n",
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32),\n",
    "    logits=decoder_logits,\n",
    ")\n",
    "\n",
    "#loss function\n",
    "loss = tf.reduce_mean(stepwise_cross_entropy)\n",
    "#train it \n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# send 10 sequences into encoder at one time\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create batches with size of batch_size\n",
    "def create_batches(data, batch_size):\n",
    "    # stores batches\n",
    "    batches = []\n",
    "    # stores last batch beginning index\n",
    "    prev_batch_begin = 0\n",
    "    \n",
    "    for j in range(0, len(data)):\n",
    "        if j % batch_size ==0 and j != 0:\n",
    "            batches.append(data[prev_batch_begin:j])\n",
    "            prev_batch_begin = j\n",
    "            \n",
    "    # put the rest of it in another batch\n",
    "    if prev_batch_begin != j:\n",
    "        batches.append(data[prev_batch_begin:j])\n",
    "        \n",
    "    return batches\n",
    "\n",
    "# encoder inputs devided into batches\n",
    "source_batches = create_batches(source_data, batch_size)\n",
    "\n",
    "# decoder targets devided into batches\n",
    "target_batches = create_batches(target_data, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A bemeneti adatokból és az elvárt kimenetekből legyártja a batch_size-nak megfelelő méretű batcheket. \n",
    "\n",
    "Maga a batch szekvenciák kötegét jelenti, hogy egyszerre hány input sort adunk be a rendszerünknek. Ezért fontos, hogy mind az encoder bemenetén, mind a decoder kimenetén azonos méretű batchek legyenek. Emiatt hívjuk meg a source_data és target_data-ra is egyaránt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def next_feed(batch_num, source_batches, target_batches):\n",
    "    # get transpose of source_batches[batch_num]\n",
    "    encoder_inputs_, encoder_input_lengths_ = helpers.batch(source_batches[batch_num])\n",
    "    \n",
    "    # get max input sequence length\n",
    "    max_input_length = max(encoder_input_lengths_)\n",
    "    \n",
    "    # target word is max character_changing_num character longer than source word \n",
    "    # get transpose of target_batches[i] and put an EOF and PAD at the end\n",
    "    decoder_targets_, _ = helpers.batch(\n",
    "            [(sequence) + [EOS] + [PAD] * ((max_input_length + character_changing_num - 1) - len(sequence))  for sequence in target_batches[batch_num]]\n",
    "    )\n",
    "   \n",
    "    return {\n",
    "        encoder_inputs: encoder_inputs_,\n",
    "        encoder_inputs_length: encoder_input_lengths_,\n",
    "        decoder_targets: decoder_targets_,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Mivel a forrás szóalak hossza nem feltétlenül egyezik meg a cél szóalak hosszával, ezért fontos hogy lehetővé tegyük a rendszer számára, hogy további karaktereket fűzhessen az eredetihez. Azt, hogy hány karakterrel lehet hosszabb a képzett szó (cél szó) az character_changing_num változó definiálja.\n",
    "\n",
    "    Ha a character_changing_num = 10 ez azt jelenti, hogy 9 karakterben térhet el az eredeti szóalaktól, mivel a szavak végére +1 karakterként odatesszük az EOF karaktert, hogy jelezzük a decodernek, hogy befejezheti a feldolgozást.\n",
    "    \n",
    "Az ehhez szükséges padding karakterek számának kiszámolásához megkeressük a legnagyobb bemeneti szekvencia hosszát, amit a max_input_length változóban tárolunk el. Ezután a legnagyobb bemeneti szekvencia hosszához hozzáadjuk a (character_changing_num-1) értéket (-1 mert EOF külön hozzáadva) és kivonjuk belőle az aktuális szekvencia hosszát. Ezzel az ettől való eltéréseket 0-val töltjük fel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "  minibatch loss: 3.658271312713623\n",
      "  sample 1:\n",
      "    input     > [ 2  3  4  5  6  7  2  3  8  9  6  7  2  3  4  5  6  7  8  9 10 11 12 13  6\n",
      " 14]\n",
      "    predicted > [40  1 28 28  7 15  3 23  4 23 41 11 15  4 23 11 41 11 15  4 23 11 41 11 15\n",
      "  4 23 11 29 23 11 41 11 15  4 23]\n",
      "  sample 2:\n",
      "    input     > [ 2  5  2 10 13 12 16 15 17  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0]\n",
      "    predicted > [23  1 41 37  1  1 28 28 39 28 39 28 23  7  4 23 11 41 11  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  sample 3:\n",
      "    input     > [ 2 11  2 12 13 14 11 20  8 15  2 15  6  8  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0]\n",
      "    predicted > [39 28 39 28 23  7  4  4 23 11 41 11 15  4 23 11 41 11 15  4 23 11  4 23  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "batch 100\n",
      "  minibatch loss: 2.5866785049438477\n",
      "  sample 1:\n",
      "    input     > [ 2 15  4 12  5 13  7  2  3  4  9  6  7 31 19  3  2  7  6 23 15  5 14  0  0\n",
      "  0]\n",
      "    predicted > [ 2  2  2  2  3  2  5  2  5 13  5  6  6 14 19 19 19 19 19  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  sample 2:\n",
      "    input     > [ 2 11  2 15  8 12  5 13  7 14 30 32  5 12  5 22  6  8  0  0  0  0  0  0  0\n",
      "  0]\n",
      "    predicted > [ 2  2  2  2  5  2  5 13 14  5  6  6 14 19 19 19 19  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  sample 3:\n",
      "    input     > [17 19 14 17 38 14 31 19  6 15 10 11  3  3 11  0  0  0  0  0  0  0  0  0  0\n",
      "  0]\n",
      "    predicted > [17 17 17 14 14 19 14 19 19 19  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "batch 200\n",
      "  minibatch loss: 2.2188973426818848\n",
      "  sample 1:\n",
      "    input     > [ 2  9  2  3  8  5  6  7 10 25  8  6 15 19 15  8 25 23 22 15  0  0  0  0  0\n",
      "  0]\n",
      "    predicted > [ 2  3  4  2 12  2  9  7  9  7  7 10 19 22 19 22 15 15 15  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  sample 2:\n",
      "    input     > [ 2  3  4 12  9 13  7  2 11 32 19 11 26 16 15 19 15 15 19 15 19 14  0  0  0\n",
      "  0]\n",
      "    predicted > [ 2  3  4  2  9  9  9  7  7 10 19 19 22 19 22 15 15 15  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  sample 3:\n",
      "    input     > [ 2  6 14 11  2  6  7 11 14  5  6 27 12  5  6 15 23 22  6  5 31  0  0  0  0\n",
      "  0]\n",
      "    predicted > [ 2  3  4  2  6 14 12 14 12  5 22 19 22 15 19 22 15 15  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "batch 300\n",
      "  minibatch loss: 2.12213397026062\n",
      "  sample 1:\n",
      "    input     > [ 2 16  4 12  5 13  7  2  3  8  9  6  7 20  8 10 10 25  2 27 19 28 15 10 19\n",
      " 15 19 14]\n",
      "    predicted > [ 2  3  4  5 12  5 13  7  2 13 13 13 12 19 19 19 19 19 22 15 15 15 15  1 14\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  sample 2:\n",
      "    input     > [ 2 15  4  5  6 14  2  6 14 11 20  2 14  2 12  6 11 14  0  0  0  0  0  0  0\n",
      "  0  0  0]\n",
      "    predicted > [ 2  6  4  5  6  6  7  6 19 19 19 15 15 15 15  1  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  sample 3:\n",
      "    input     > [ 2 15  8 12  5 13  7  2  3  4 12  9 13 14 14 29 22 31  8  6 23 22  6 23 15\n",
      "  5 14  0]\n",
      "    predicted > [ 2  3  4 12  5 12  2 12  2 13 13  5 13  5  7  5  7 19 22 15 15 15 15 15  1\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "batch 400\n",
      "  minibatch loss: 2.0991458892822266\n",
      "  sample 1:\n",
      "    input     > [ 2 12 13  7 11  2  3  4 12  9 13  7  8 22 22 19 15  6 19 15 19 14  0  0  0\n",
      "  0  0  0]\n",
      "    predicted > [ 2 12  2 12 13 13 13 13 13  7 13 19 19 19 19 19 19 19 15 15 15 15  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  sample 2:\n",
      "    input     > [ 2 11  2 15  8 12  5 13  7  2 25  5  6  5 10 16 15  2  6  8  0  0  0  0  0\n",
      "  0  0  0]\n",
      "    predicted > [ 2 12  2 12  5  5  5  7  5  5  7  2 15  5 15 15 15 15  1  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  sample 3:\n",
      "    input     > [ 2  3  8  5  6  7  2 16  4 12  5 13  7 22  2 26 16 15 28 29 14  0  0  0  0\n",
      "  0  0  0]\n",
      "    predicted > [ 2  3  4  5  6  7  2  2  2  5 13  7  5  7 19 19 19 15 15 15 15 15  1  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "batch 500\n",
      "  minibatch loss: 1.8622280359268188\n",
      "  sample 1:\n",
      "    input     > [ 2 15  4 12  5 13  7  2 16  8 12  5 13  7 19 22 10  5  7  5 12  6 23 15  5\n",
      " 14]\n",
      "    predicted > [ 2 15  8 12  5 13  7  2  3  8 12  5 13  7  5  7  2 15 22  5 15 15 15 15  1\n",
      "  1  0  0  0  0  0  0  0  0  0  0]\n",
      "  sample 2:\n",
      "    input     > [ 2  5  2 15  4 12  5 13  7 14 11 26 20  8 10 19 22 17  0  0  0  0  0  0  0\n",
      "  0]\n",
      "    predicted > [ 2 16  5  5 13  7  2  2  5 13 14 19 19 19 22 15 15 15 15  1  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  sample 3:\n",
      "    input     > [ 2  3  8 12  9 13 14  2  3  4  9  6 14 14  8 22 11 22 19  3 19 25 15 19  7\n",
      "  0]\n",
      "    predicted > [ 2  3  4 12  9 13  7  2  3  8 12  5 13 14 19 19 19 19 22 15 15 15 15  1  1\n",
      "  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "batch 600\n",
      "  minibatch loss: 1.8355501890182495\n",
      "  sample 1:\n",
      "    input     > [ 2  3  4 12  9 13  7  2 11 26 23  9  5 22 15  2 15  5 14  0  0  0  0  0  0\n",
      "  0  0  0  0  0]\n",
      "    predicted > [ 2  3  4 12  9 13  7  2  2 10  5  6 22  5 15 15 15  1  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  sample 2:\n",
      "    input     > [ 2  3  8 12  5 13 14  2 15  8 12  5 13  7  8  6  3  2 15  5  7  0  0  0  0\n",
      "  0  0  0  0  0]\n",
      "    predicted > [ 2  3  8 12  5 13 14  2  3  8 12  5 13 14  5  7  2 15 15 15 15 15  1  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  sample 3:\n",
      "    input     > [17 30  7 17 36  7 14 29 12  7  5 14  6 23 22  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0]\n",
      "    predicted > [17 17 17 17 14  7 14  7  2 15  2  5  6 14  1  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "batch 700\n",
      "  minibatch loss: 1.7749346494674683\n",
      "  sample 1:\n",
      "    input     > [ 2  3  8  9  6  7  2 16  4  5  6 14  5 26 26  5  6 23 22 15 29 14  0  0  0\n",
      "  0  0  0]\n",
      "    predicted > [ 2  3  4  5  6  7  2  3  4  5  6  7  2 15  6 15 23 23  5 15  1  1  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  sample 2:\n",
      "    input     > [ 2  3  8 12  5 13 14  2  3  4 12  9 13 14 13 15 21  7  0  0  0  0  0  0  0\n",
      "  0  0  0]\n",
      "    predicted > [ 2  3  8 12  5 13 14  2  3  8 12  5 13 14 22  2 22  5 15 15 15  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  sample 3:\n",
      "    input     > [ 2  9  2 15  4 12  5 13  7 31  2  3 23 25  5 15 15  0  0  0  0  0  0  0  0\n",
      "  0  0  0]\n",
      "    predicted > [ 2  2  2 12  5 13  7  2  5  5  6  5  6 15 15 15 15  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "batch 800\n",
      "  minibatch loss: 1.551725149154663\n",
      "  sample 1:\n",
      "    input     > [ 2 16  4  5  6  7  2  3  8 12  5 13  7 18 33 15 10 13  6 14  0  0  0  0  0\n",
      "  0]\n",
      "    predicted > [ 2 16  4  5  6  7  2  3  8 12  5 13  7  2 15 22  5 15 15 15  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  sample 2:\n",
      "    input     > [ 2 10  2 15  8 12  5 13  7 14 21 26 18 19 15  0  0  0  0  0  0  0  0  0  0\n",
      "  0]\n",
      "    predicted > [ 2  2  2  2 12  5 13  7  5  5 22 23 15 15 15  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  sample 3:\n",
      "    input     > [ 2  6  7 11  2 16  8 12  5 13  7 32 22  5  3  5 22  6 29  6 14  0  0  0  0\n",
      "  0]\n",
      "    predicted > [ 2  6  7  2 15  8 12  5 13  7  7  2  2 15 22  5  6 15 15 15  1  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "batch 900\n",
      "  minibatch loss: 1.5332350730895996\n",
      "  sample 1:\n",
      "    input     > [ 2 15  4 12  5 13 14  2 16  8 12  5 13  7 26  2 12 14  5 22  6 23 22  0  0\n",
      "  0  0  0  0]\n",
      "    predicted > [ 2 15  4 12  5 13 14  2  3  8 12  5 13 14  5  7  2  2 15 15  5 15  5 14  1\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  sample 2:\n",
      "    input     > [ 2  3  4 12  9 13  7  2 16  8 12  5 13  7 14 11  6  4 19 25 15 19 15 15 19\n",
      " 15 19 14  0]\n",
      "    predicted > [ 2  3  4 12  9 13  7  2  3  8 12  5 13  7 31 19 19 19 19 19 19 15 15 15 15\n",
      " 14 14  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  sample 3:\n",
      "    input     > [ 2  3  4 12  5 13 14  2  3  4 12  9 13 14 15 12  2  6 10 25 26 22  2  6 15\n",
      " 23 22 10 25]\n",
      "    predicted > [ 2  3  4 12  5 13 14  2  3  4 12  5 13 14 14  2 20  3 19 25 25 25 25 15  6\n",
      " 15  6  6 14  1  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "batch 1000\n",
      "  minibatch loss: 1.4091651439666748\n",
      "  sample 1:\n",
      "    input     > [ 2  3  8  9  6  7  2 15  8 12  5 13  7 19 22 12  2  3  2  7 15  2 15 15 29\n",
      " 14]\n",
      "    predicted > [ 2  3  8  9  6  7  2  3  8 12  5 13  7  7 22 12  8 15 15 23 23 23 23 15 15\n",
      " 14 14  1  0  0  0  0  0  0  0  0]\n",
      "  sample 2:\n",
      "    input     > [ 2  3  4  5  6  7  2  3  4 12  9 13 14 20  8  3  2 10 25 15  2 22 29  6 14\n",
      "  0]\n",
      "    predicted > [ 2  3  4  5  6  7  2  3  8 12  9 13  7  2  2  3  8 25 22 25 22 15 15 15 15\n",
      " 14 14  1  0  0  0  0  0  0  0  0]\n",
      "  sample 3:\n",
      "    input     > [ 2  3  4 12  5 13 14  2 16  4  5  6 14 14  5  6 25 19 12 20 23 22 10 25  0\n",
      "  0]\n",
      "    predicted > [ 2  3  4 12  5 13 14  2 16  4  5  6  7 14 19 19 19 19 15 15 15 15 14  1  1\n",
      "  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "batch 1100\n",
      "  minibatch loss: 1.4562506675720215\n",
      "  sample 1:\n",
      "    input     > [ 2  5  2 15  4  5  6  7 31 19  3 10 25 23 31  5 25 30  0  0  0  0  0  0  0]\n",
      "    predicted > [ 2  5  2 16  4  5  6  7  2 20 19 19 19 19 19 25 25 15 15 15 14  1  1  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0]\n",
      "  sample 2:\n",
      "    input     > [ 2 12 13  7 11  2 20 32  5 31 32 23 25  6  5 15  5 14  0  0  0  0  0  0  0]\n",
      "    predicted > [ 2 12 13  7 11  2  2 22 22 22  5 22  5  6  6  1  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0]\n",
      "  sample 3:\n",
      "    input     > [ 2  3  4  5  6 14  2 12 13  7 11 18  2 12 31  5  6  8 25 23 22  5 14  0  0]\n",
      "    predicted > [ 2  3  4  5  6  7  2  3  8 12  9 13  7 19 19 25 25  2 15 15  2  5  5  6  1\n",
      "  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "batch 1200\n",
      "  minibatch loss: 1.4665549993515015\n",
      "  sample 1:\n",
      "    input     > [ 2 20  2  3  4 12  9 13 14 20 19 12 32 29 20 23 22  2  6  7 30  0  0  0  0\n",
      "  0  0]\n",
      "    predicted > [ 2  2  2  3  4 12  5 13 14 19 19 19 19 19 19 22 22 22 15 15 15  1  1  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  sample 2:\n",
      "    input     > [ 2  3  4  9  6 14  2 12 13  7 11 25  2  3  4 20 23 22 15  2 31  0  0  0  0\n",
      "  0  0]\n",
      "    predicted > [ 2  3  4  9  6 14  2  3  4 12  9 13  7 10 25  2  2 15 15 23 15 15 14  1  1\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  sample 3:\n",
      "    input     > [ 2  6 14 11  2 16  4  5  6 14  6  4 24 28 15  2  6  5 31  0  0  0  0  0  0\n",
      "  0  0]\n",
      "    predicted > [ 2  6  7 11  2 16  4  5  6  7 14  8  5  6 15  6 15  6 14  1  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "batch 1300\n",
      "  minibatch loss: 1.3304332494735718\n",
      "  sample 1:\n",
      "    input     > [ 2 12 13  7 11  2 15  8  5  6  7 27  2 14  2 10 25 15  2  6  5 15  5 14  0\n",
      "  0]\n",
      "    predicted > [ 2 12 13  7 11  2 15  4  5  6  7  5  2  2  2  5  5  5  6  6 14 14  1  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  sample 2:\n",
      "    input     > [ 2 15  8  5  6  7  2 11 22 19  8  6 15 19  6 11  6 14  0  0  0  0  0  0  0\n",
      "  0]\n",
      "    predicted > [ 2 15  8  5  6  7 11 19 19 19 19 19 15 19 19  1  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  sample 3:\n",
      "    input     > [ 2 10  2 16  4  5  6  7 14  2 26 18  2 15  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0]\n",
      "    predicted > [ 2 10  2 16  2 16  5  6 14  2  2  2 15  5  1  1  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "batch 1400\n",
      "  minibatch loss: 1.3475341796875\n",
      "  sample 1:\n",
      "    input     > [ 2 20  2  3  8 12  5 13  7 15  2  6 23  9 10  5 22  2  6  7 30  0  0  0  0\n",
      "  0  0  0]\n",
      "    predicted > [ 2  3  2  3  8 12  5 13  7  2 10  5  2  2  2  5  5  5  5  5  1  1  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  sample 2:\n",
      "    input     > [ 2  5  2 15  4 12  5 13 14  7 19  3 19  6 19 12 23 22 30  0  0  0  0  0  0\n",
      "  0  0  0]\n",
      "    predicted > [ 2  5  2 15  4 12  5 13  7  2 20 19 19 19 19 19 19 19 15 19  1  1  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  sample 3:\n",
      "    input     > [ 2 15  8  5  6  7  2  3  4 12  5 13  7 26  8 23 22  6 23  6 14  0  0  0  0\n",
      "  0  0  0]\n",
      "    predicted > [ 2 15  8  5  6  7  2  3  8 12  5 13  7  7  8  5 22 23 23  5  5  5  1  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "batch 1500\n",
      "  minibatch loss: 1.2419434785842896\n",
      "  sample 1:\n",
      "    input     > [ 2 15  8  5  6  7  2 16  4  5  6  7 20 19 25 19 15  6 11  6 14  0  0  0  0\n",
      "  0]\n",
      "    predicted > [ 2 16  8  5  6  7  2 16  4  5  6  7 19 19 19 19 15 15 15 15 14  1  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  sample 2:\n",
      "    input     > [ 2 15  4  5  6 14  2  3  4  5  6  7 22  2 26 16 15  2  6 11 14  0  0  0  0\n",
      "  0]\n",
      "    predicted > [ 2 15  4  5  6 14  2  3  4  5  6  7  2  2  2  2  2 15 15 15 29  1  1  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  sample 3:\n",
      "    input     > [ 2 12 13 14 11  2  5 19 22 20  5  6 10 25  5 22  6  5  7  0  0  0  0  0  0\n",
      "  0]\n",
      "    predicted > [ 2 12 13 14 11  2 11 19 19 19 22 22 22 22 23  6  6  1  1  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "batch 1600\n",
      "  minibatch loss: 1.386610746383667\n",
      "  sample 1:\n",
      "    input     > [ 2 15  8 12  5 13 14  2  6 14 11 14  5  6 27  8 12 31 23 22  6 23  7  0  0\n",
      "  0  0]\n",
      "    predicted > [ 2 15  8 12  5 13 14  2 15  4 12  5 13 14 19 22 22 22 22  5  6  5  1  1  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  sample 2:\n",
      "    input     > [ 2  3  8 12  5 13 14  2 16  4  5  6 14 32  5 12  5 15 20 23 22  5  7  0  0\n",
      "  0  0]\n",
      "    predicted > [ 2  3  8 12  5 13 14  2 16  4  5  6  7 14  2  2  2  5  5 22  5 22  2  1  1\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  sample 3:\n",
      "    input     > [17 23 14 17 37  7 20 23  6 14  5 10 15  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0]\n",
      "    predicted > [17 17 17 17  7  7 14  2 15  6 14 14 14 14  1  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "batch 1700\n",
      "  minibatch loss: 1.2521146535873413\n",
      "  sample 1:\n",
      "    input     > [ 2  3  4  5  6  7  2  6 14 11 19 22 12  5  6 15 29  6 14  0  0  0  0  0  0\n",
      "  0]\n",
      "    predicted > [ 2  3  4  5  6  7  2  6  7 11 19 19 22 22 22  6  6  6  1  1  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  sample 2:\n",
      "    input     > [17 40  7 17 18  7  2  6  3  5 22  5 14 14  2 22  0  0  0  0  0  0  0  0  0\n",
      "  0]\n",
      "    predicted > [17 17 17  7 17  7 14  2 15  2 15 14 14 14  1  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  sample 3:\n",
      "    input     > [ 2 15  4 12  5 13 14  2 11 20 11  3  8  3 32 19 10 25 11 22  6 11 22  0  0\n",
      "  0]\n",
      "    predicted > [ 2 15  4 12  5 13 14  2 20 10 11 19 19 19 19 19 19 19 19 19  1  1  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "batch 1800\n",
      "  minibatch loss: 1.014326810836792\n",
      "  sample 1:\n",
      "    input     > [ 2  3  8 12  5 13  7  2 16  8 12  5 13  7 27 19 22 15 19 10 25  8 15 19 14]\n",
      "    predicted > [ 2  3  8 12  5 13  7  2 15  8 12  5 13  7 19 19 19 19 19 19 15 15 15 15 19\n",
      " 14  1  0  0  0  0  0  0  0  0]\n",
      "  sample 2:\n",
      "    input     > [ 2 15  8 12  5 13 14  2  6  7 11 14 21 26  6 11  7  0  0  0  0  0  0  0  0]\n",
      "    predicted > [ 2 15  8 12  5 13 14  2  5  6 14 19 19 19 15  6 14  1  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0]\n",
      "  sample 3:\n",
      "    input     > [ 2  3  4  5  6 14  2 15  8 12  5 13  7 27 19 22 19  7 19 14  0  0  0  0  0]\n",
      "    predicted > [ 2  3  4  5  6 14  2 15  8 12  5 13  7 19 19 19 19 19 15 15 15 19  1  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "batch 1900\n",
      "  minibatch loss: 0.960521936416626\n",
      "  sample 1:\n",
      "    input     > [ 2  5  2 16  4  5  6 14 18 19 22  4 32 19  6 18  2  3  4 30  0  0  0  0]\n",
      "    predicted > [ 2  5  2 16  4  5  6  7  2 20 19 19 19 19 19 25 22 22  6  6  6  1  1  0  0\n",
      "  0  0  0  0  0  0  0  0  0]\n",
      "  sample 2:\n",
      "    input     > [ 2  3  4 12  5 13  7  2  3  4  9  6  7  2 14  2 12 15  5 14  0  0  0  0]\n",
      "    predicted > [ 2  3  4 12  5 13  7  2  3  4  5  6  7  2  2  2 15 15 29  1  1  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0]\n",
      "  sample 3:\n",
      "    input     > [ 2 15  8 12  5 13  7  2 11 20  8 22 23  3 16 15  2  6 23 15  5 14  0  0]\n",
      "    predicted > [ 2 15  8 12  5 13  7  2  2 10 25  2  2  2 15  6  6  6  6  1  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_track = []\n",
    "\n",
    "try:\n",
    "    # get every batches and train the model on it\n",
    "    for batch_num in range(0, len(source_batches)):\n",
    "        fd = next_feed(batch_num, source_batches, target_batches)\n",
    "   \n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        loss_track.append(l)\n",
    "        \n",
    "        if batch_num == 0 or batch_num % batches_in_epoch == 0:\n",
    "            print('batch {}'.format(batch_num))\n",
    "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "            predict_ = sess.run(decoder_prediction, fd)\n",
    "            for i, (inp, pred) in enumerate(zip(fd[encoder_inputs].T, predict_.T)):\n",
    "                print('  sample {}:'.format(i + 1))\n",
    "                print('    input     > {}'.format(inp))\n",
    "                print('    predicted > {}'.format(pred))\n",
    "                if i >= 2:\n",
    "                    break\n",
    "            print()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
