{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "import helpers\n",
    "import operator\n",
    "\n",
    "# x\n",
    "input_data = []\n",
    "# y\n",
    "target_data = []\n",
    "\n",
    "# betűk kódolása\n",
    "alphabet = dict()\n",
    "# forrás és cél morfológiai tagek kódolása\n",
    "morphological_tags = dict()\n",
    "\n",
    "# kezdőkarakter\n",
    "BOS = 0\n",
    "# zárókarakter\n",
    "EOS = 1\n",
    "\n",
    "# beolvassa és feldarabolja input adatot\n",
    "with open('data2.tsv','r') as input:\n",
    "    for line in input:\n",
    "        data_line_ = line.strip('\\n').split('\\t')\n",
    "        \n",
    "        # szavak kódolása számvektorokká    \n",
    "        for item in range(0,4):          \n",
    "            coded_word = []\n",
    "            \n",
    "            if item == 1 or item == 3:\n",
    "                # forrás és cél szóalak kódolása\n",
    "                for character in data_line_[item]:\n",
    "                    index = alphabet.setdefault(character, len(alphabet) + 2)\n",
    "                    coded_word.append(index)\n",
    "            else:\n",
    "                # morfológiai tag-ek kódolása\n",
    "                index = morphological_tags.setdefault(data_line_[item], len(morphological_tags) + 2)\n",
    "                coded_word.append(index)\n",
    "            \n",
    "            data_line_[item] = coded_word\n",
    "        \n",
    "        source = []\n",
    "        \n",
    "        for i in data_line_[0]:\n",
    "            source.append(i)\n",
    "            \n",
    "        for i in data_line_[2]:\n",
    "            source.append(i)\n",
    "            \n",
    "        for i in data_line_[1]:\n",
    "            source.append(i)\n",
    "        \n",
    "        input_data.append(source)\n",
    "        # cél szóalak\n",
    "        target = []\n",
    "        \n",
    "        for i in data_line_[0]:\n",
    "            target.append(i)\n",
    "            \n",
    "        for i in data_line_[2]:\n",
    "            target.append(i)\n",
    "        \n",
    "        for i in data_line_[3]:\n",
    "            target.append(i)\n",
    "           \n",
    "        target_data.append(target)\n",
    "        \n",
    "                \n",
    "# -----------------------------\n",
    "\n",
    "tf.reset_default_graph() #Clears the default graph stack and resets the global default graph.\n",
    "sess = tf.InteractiveSession() #initializes a tensorflow session\n",
    "\n",
    "PAD = 0\n",
    "EOS = 1\n",
    "\n",
    "# calculate vocab_size (max(alphabet,morphological_tags))\n",
    "max_alphabet = alphabet[max(alphabet.items(), key=operator.itemgetter(1))[0]]\n",
    "max_morphological_tags = morphological_tags[max(morphological_tags.items(), key=operator.itemgetter(1))[0]]\n",
    "\n",
    "vocab_size = max([max_alphabet,max_morphological_tags]) + 1 # hány különböző karakterből áll a szó max\n",
    "input_embedding_size = 30 #character length (hány karakterból áll a szó)\n",
    "\n",
    "encoder_hidden_units = 20 #num neurons\n",
    "decoder_hidden_units = encoder_hidden_units * 2 #in original paper, they used same number of neurons for both encoder\n",
    "#and decoder, but we use twice as many so decoded output is different, the target value is the original input \n",
    "#in this example\n",
    "\n",
    "#input placehodlers\n",
    "encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "#contains the lengths for each of the sequence in the batch, we will pad so all the same\n",
    "#if you don't want to pad, check out dynamic memory networks to input variable length sequences\n",
    "encoder_inputs_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='encoder_inputs_length')\n",
    "decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')\n",
    "\n",
    "\n",
    "#randomly initialized embedding matrrix that can fit input sequence\n",
    "#used to convert sequences to vectors (embeddings) for both encoder and decoder of the right size\n",
    "#reshaping is a thing, in TF you gotta make sure you tensors are the right shape (num dimensions)\n",
    "embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)\n",
    "\n",
    "#this thing could get huge in a real world application\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)\n",
    "\n",
    "# encoder definiálás\n",
    "encoder_cell = tf.contrib.rnn.LSTMCell(encoder_hidden_units)\n",
    "\n",
    "\n",
    "# bidirectional encodre funkció definiálása (backpropagation)\n",
    "((encoder_fw_outputs,\n",
    "  encoder_bw_outputs),\n",
    " (encoder_fw_final_state,\n",
    "  encoder_bw_final_state)) = (\n",
    "    tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,\n",
    "                                    cell_bw=encoder_cell,\n",
    "                                    inputs=encoder_inputs_embedded,\n",
    "                                    sequence_length=encoder_inputs_length,\n",
    "                                    dtype=tf.float32, time_major=True)\n",
    "    )\n",
    "\n",
    "\n",
    "#Concatenates tensors along one dimension.\n",
    "encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2)\n",
    "\n",
    "#letters h and c are commonly used to denote \"output value\" and \"cell state\". \n",
    "#http://colah.github.io/posts/2015-08-Understanding-LSTMs/ \n",
    "#Those tensors represent combined internal state of the cell, and should be passed together. \n",
    "\n",
    "encoder_final_state_c = tf.concat(\n",
    "    (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n",
    "\n",
    "encoder_final_state_h = tf.concat(\n",
    "    (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "\n",
    "#TF Tuple used by LSTM Cells for state_size, zero_state, and output state.\n",
    "encoder_final_state = tf.contrib.rnn.LSTMStateTuple(\n",
    "    c=encoder_final_state_c,\n",
    "    h=encoder_final_state_h\n",
    ")\n",
    "\n",
    "decoder_cell = tf.contrib.rnn.LSTMCell(decoder_hidden_units)\n",
    "\n",
    "#we could print this, won't need\n",
    "encoder_max_time, batch_size = tf.unstack(tf.shape(encoder_inputs))\n",
    "\n",
    "decoder_lengths = encoder_inputs_length + 10\n",
    "# +2 additional steps, +1 leading <EOS> token for decoder inputs\n",
    "\n",
    "#manually specifying since we are going to implement attention details for the decoder in a sec\n",
    "#weights\n",
    "W = tf.Variable(tf.random_uniform([decoder_hidden_units, vocab_size], -1, 1), dtype=tf.float32)\n",
    "#bias\n",
    "b = tf.Variable(tf.zeros([vocab_size]), dtype=tf.float32)\n",
    "\n",
    "#create padded inputs for the decoder from the word embeddings\n",
    "\n",
    "#were telling the program to test a condition, and trigger an error if the condition is false.\n",
    "assert EOS == 1 and PAD == 0\n",
    "\n",
    "eos_time_slice = tf.ones([batch_size], dtype=tf.int32, name='EOS')\n",
    "pad_time_slice = tf.zeros([batch_size], dtype=tf.int32, name='PAD')\n",
    "\n",
    "#retrieves rows of the params tensor. The behavior is similar to using indexing with arrays in numpy\n",
    "eos_step_embedded = tf.nn.embedding_lookup(embeddings, eos_time_slice)\n",
    "pad_step_embedded = tf.nn.embedding_lookup(embeddings, pad_time_slice)\n",
    "\n",
    "#manually specifying loop function through time - to get initial cell state and input to RNN\n",
    "#normally we'd just use dynamic_rnn, but lets get detailed here with raw_rnn\n",
    "\n",
    "#we define and return these values, no operations occur here\n",
    "def loop_fn_initial():\n",
    "    initial_elements_finished = (0 >= decoder_lengths)  # all False at the initial step\n",
    "    #end of sentence\n",
    "    initial_input = eos_step_embedded\n",
    "    #last time steps cell state\n",
    "    initial_cell_state = encoder_final_state\n",
    "    #none\n",
    "    initial_cell_output = None\n",
    "    #none\n",
    "    initial_loop_state = None  # we don't need to pass any additional information\n",
    "    return (initial_elements_finished,\n",
    "            initial_input,\n",
    "            initial_cell_state,\n",
    "            initial_cell_output,\n",
    "            initial_loop_state)\n",
    "\n",
    "\n",
    "#attention mechanism --choose which previously generated token to pass as input in the next timestep\n",
    "def loop_fn_transition(time, previous_output, previous_state, previous_loop_state):\n",
    "\n",
    "    \n",
    "    def get_next_input():\n",
    "        #dot product between previous ouput and weights, then + biases\n",
    "        output_logits = tf.add(tf.matmul(previous_output, W), b)\n",
    "        #Logits simply means that the function operates on the unscaled output of \n",
    "        #earlier layers and that the relative scale to understand the units is linear. \n",
    "        #It means, in particular, the sum of the inputs may not equal 1, that the values are not probabilities \n",
    "        #(you might have an input of 5).\n",
    "        #prediction value at current time step\n",
    "        \n",
    "        #Returns the index with the largest value across axes of a tensor.\n",
    "        prediction = tf.argmax(output_logits, axis=1)\n",
    "        #embed prediction for the next input\n",
    "        next_input = tf.nn.embedding_lookup(embeddings, prediction)\n",
    "        return next_input\n",
    "    \n",
    "    \n",
    "    elements_finished = (time >= decoder_lengths) # this operation produces boolean tensor of [batch_size]\n",
    "                                                  # defining if corresponding sequence has ended\n",
    "\n",
    "    \n",
    "    \n",
    "    #Computes the \"logical and\" of elements across dimensions of a tensor.\n",
    "    finished = tf.reduce_all(elements_finished) # -> boolean scalar\n",
    "    #Return either fn1() or fn2() based on the boolean predicate pred.\n",
    "    input = tf.cond(finished, lambda: pad_step_embedded, get_next_input)\n",
    "    \n",
    "    #set previous to current\n",
    "    state = previous_state\n",
    "    output = previous_output\n",
    "    loop_state = None\n",
    "\n",
    "    return (elements_finished, \n",
    "            input,\n",
    "            state,\n",
    "            output,\n",
    "            loop_state)\n",
    "\n",
    "def loop_fn(time, previous_output, previous_state, previous_loop_state):\n",
    "    if previous_state is None:    # time == 0\n",
    "        assert previous_output is None and previous_state is None\n",
    "        return loop_fn_initial()\n",
    "    else:\n",
    "        return loop_fn_transition(time, previous_output, previous_state, previous_loop_state)\n",
    "\n",
    "#Creates an RNN specified by RNNCell cell and loop function loop_fn.\n",
    "#This function is a more primitive version of dynamic_rnn that provides more direct access to the \n",
    "#inputs each iteration. It also provides more control over when to start and finish reading the sequence, \n",
    "#and what to emit for the output.\n",
    "#ta = tensor array\n",
    "decoder_outputs_ta, decoder_final_state, _ = tf.nn.raw_rnn(decoder_cell, loop_fn)\n",
    "decoder_outputs = decoder_outputs_ta.stack()\n",
    "\n",
    "decoder_outputs\n",
    "\n",
    "\n",
    "#to convert output to human readable prediction\n",
    "#we will reshape output tensor\n",
    "\n",
    "#Unpacks the given dimension of a rank-R tensor into rank-(R-1) tensors.\n",
    "#reduces dimensionality\n",
    "decoder_max_steps, decoder_batch_size, decoder_dim = tf.unstack(tf.shape(decoder_outputs))\n",
    "#flettened output tensor\n",
    "decoder_outputs_flat = tf.reshape(decoder_outputs, (-1, decoder_dim))\n",
    "#pass flattened tensor through decoder\n",
    "decoder_logits_flat = tf.add(tf.matmul(decoder_outputs_flat, W), b)\n",
    "#prediction vals\n",
    "decoder_logits = tf.reshape(decoder_logits_flat, (decoder_max_steps, decoder_batch_size, vocab_size))\n",
    "\n",
    "#final prediction\n",
    "decoder_prediction = tf.argmax(decoder_logits, 2)\n",
    "\n",
    "#cross entropy loss\n",
    "#one hot encode the target values so we don't rank just differentiate\n",
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32),\n",
    "    logits=decoder_logits,\n",
    ")\n",
    "\n",
    "#loss function\n",
    "loss = tf.reduce_mean(stepwise_cross_entropy)\n",
    "#train it \n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "batches = input_data\n",
    "\n",
    "# 10-es batchek legyártása forrás adatból\n",
    "input_batches = []\n",
    "ten_sequence2 = []\n",
    "for j in range(0,len(input_data)):\n",
    "    if j % 10 == 0:\n",
    "        input_batches.append(ten_sequence2)\n",
    "        ten_sequence2 = []\n",
    "        ten_sequence2.append(input_data[j])\n",
    "    else:\n",
    "        ten_sequence2.append(input_data[j])\n",
    "\n",
    "if ten_sequence2 != '': \n",
    "    input_batches.append(ten_sequence2)\n",
    "    \n",
    "batches = input_batches[1:len(input_batches)] \n",
    "\n",
    "######\n",
    "\n",
    "# 10-es batchek legyártása cél adatból\n",
    "target_batches = []\n",
    "ten_sequence = []\n",
    "for j in range(0,len(target_data)):\n",
    "    if j % 10 == 0:\n",
    "        target_batches.append(ten_sequence)\n",
    "        ten_sequence = []\n",
    "        ten_sequence.append(target_data[j])\n",
    "    else:\n",
    "        ten_sequence.append(target_data[j])\n",
    "\n",
    "if ten_sequence != '': \n",
    "    target_batches.append(ten_sequence)\n",
    "    \n",
    "target_batches = target_batches[1:len(target_batches)] \n",
    "\n",
    "######\n",
    "\n",
    "def next_feed(i,batches,target_batches):\n",
    "    batch = batches[i]\n",
    "    \n",
    "    encoder_inputs_, encoder_input_lengths_ = helpers.batch(batch)\n",
    "    \n",
    "    \n",
    "    if len(max(batch,key=len)) < len(max(target_batches[i],key=len)):\n",
    "        decoder_targets_, _ = helpers.batch(\n",
    "            [(sequence) + [EOS] + [PAD] * (9-(len(max(target_batches[i],key=len))-len(max(batch,key=len)))) for sequence in target_batches[i]]\n",
    "        )\n",
    "    else:\n",
    "        decoder_targets_, _ = helpers.batch(\n",
    "            [(sequence) + [EOS] + [PAD] * (9+(len(max(batch,key=len))-len(max(target_batches[i],key=len)))) for sequence in target_batches[i]]\n",
    "        )\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        encoder_inputs: encoder_inputs_,\n",
    "        encoder_inputs_length: encoder_input_lengths_,\n",
    "        decoder_targets: decoder_targets_,\n",
    "    }\n",
    "\n",
    "\n",
    "loss_track = []\n",
    "\n",
    "max_batches = 3001\n",
    "batches_in_epoch = 10\n",
    "\n",
    "try:\n",
    "    #for batch in range(max_batches):\n",
    "    for batch in range(0,len(batches)):\n",
    "        i = batch\n",
    "        \n",
    "        fd = next_feed(i,batches,target_batches)\n",
    "   \n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        loss_track.append(l)\n",
    "        \n",
    "        '''\n",
    "        if batch == 0 or batch % batches_in_epoch == 0:\n",
    "            print('batch {}'.format(batch))\n",
    "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "            predict_ = sess.run(decoder_prediction, fd)\n",
    "            for i, (inp, pred) in enumerate(zip(fd[encoder_inputs].T, predict_.T)):\n",
    "                print('  sample {}:'.format(i + 1))\n",
    "                print('    input     > {}'.format(inp))\n",
    "                print('    predicted > {}'.format(pred))\n",
    "                if i >= 2:\n",
    "                    break\n",
    "            print()\n",
    "        ''' \n",
    "        print('batch {}'.format(batch))\n",
    "        print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "        predict_ = sess.run(decoder_prediction, fd)\n",
    "        for i, (inp, pred) in enumerate(zip(fd[encoder_inputs].T, predict_.T)):\n",
    "            print('  sample {}:'.format(i + 1))\n",
    "            print('    input     > {}'.format(inp))\n",
    "            print('    predicted > {}'.format(pred))\n",
    "            if i >= 2:\n",
    "                break\n",
    "        print()\n",
    "        \n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
